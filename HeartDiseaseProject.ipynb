{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metis Spring 2015 Data Science Bootcamp: Project Luther, Heart Disease Data\n",
    "1. Read data into a Pandas dataframe, make some plots, and prepare data for classifiers.\n",
    "2. Make some clean plots for the blog.\n",
    "3. Apply logistic regression:\n",
    "  * with scikit-learn;\n",
    "  * with statsmodels.\n",
    "  * Find out which feature correlations cause the age coefficient to be negative.\n",
    "  * Make some correlation plots between the features age, thalach, and ca.\n",
    "  * Repeat fit on a subset of features (age and thalach, separately for ca=0 and ca>0)\n",
    "  * Which features should we keep?\n",
    "    - Find the set of features that yields the best accuracy score, using cross-validation.\n",
    "    - Try eliminating features with a non-significant coefficient, one by one, while keeping the model deviance as low as possible.  We'll use this second method for the final results.\n",
    "  * Compute logistic regression model accuracy, precision and recall for the final selection of features and using the full training data set (biased estimation).\n",
    "  * Recompute accuracy, precision and recall using cross-validation to avoid bias.\n",
    "3. Apply Gaussian Naive Bayes, optimize on accuracy.\n",
    "4. Apply Support Vector Classifier, optimize on accuracy.\n",
    "5. Apply Decision Tree Classifier, optimize on accuracy.\n",
    "6. Apply Random Forest Classifier, optimize on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data, store into a dataframe, and convert categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/heart_disease_all14.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0bfd309280b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    307\u001b[0m columns = [\"age\", \"sex\", \"cp\", \"restbp\", \"chol\", \"fbs\", \"restecg\",\n\u001b[0;32m    308\u001b[0m            \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n\u001b[1;32m--> 309\u001b[1;33m \u001b[0mdf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/heart_disease_all14.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;31m# Make some plots to check the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Program Files\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'data/heart_disease_all14.csv' does not exist"
     ]
    }
   ],
   "source": [
    "'''\n",
    "List of available variables:\n",
    "\n",
    " 1. age: continuous 连续数据\n",
    " 2. sex: categorical, 分类数据 2 values {0: female, 1: male}\n",
    " 3. cp (chest pain type): 分类数据 categorical, 4 values\n",
    "    {1: typical angina, 2: atypical angina, 3: non-angina, 4: asymptomatic angina}\n",
    " 4. restbp (resting blood pressure on admission to hospital): 连续数据 continuous (mmHg)\n",
    " 5. chol (serum cholesterol level): continuous (mg/dl)\n",
    " 6. fbs (fasting blood sugar): 分类数据 categorical, 2 values {0: <= 120 mg/dl, 1: > 120 mg/dl}\n",
    " 7. restecg (resting electrocardiography): categorical, 3 values\n",
    "    {0: normal, 1: ST-T wave abnormality, 2: left ventricular hypertrophy}\n",
    " 8. thalach (maximum heart rate achieved):连续数据  continuous\n",
    " 9. exang (exercise induced angina): 分类数据 categorical, 2 values {0: no, 1: yes}\n",
    "10. oldpeak (ST depression induced by exercise relative to rest): continuous\n",
    "11. slope (slope of peak exercise ST segment): 分类数据 categorical, 3 values \n",
    "    {1: upsloping, 2: flat, 3: downsloping}\n",
    "12. ca (number of major vessels colored by fluoroscopy):离散数据  discrete (0,1,2,3)\n",
    "13. thal: 分类数据 categorical, 3 values {3: normal, 6: fixed defect, 7: reversible defect}\n",
    "14. num (diagnosis of heart disease): 分类数据 categorical, 5 values \n",
    "    {0: less than 50% narrowing in any major vessel, \n",
    "    1-4: more than 50% narrowing in 1-4 vessels}\n",
    "    \n",
    "    dummy ones 哑数据个数 = 分类数据-1\n",
    "    \n",
    "The actual number of feature variables (after converting categorical variables\n",
    "to dummy ones) is: \n",
    "1 (age) + 1 (sex) + 3 (cp) + 1 (restbp) + 1 (chol) + 1 (fbs) + 2 (restecg) + \n",
    "1 (thalach) + 1 (exang) + 1 (oldpeak) + 2 (slope) + 1 (ca) + 2 (thal) = 18\n",
    "\n",
    "The response variable (num) is categorical with 5 values, but we don't have\n",
    "enough data to predict all the categories. Therefore we'll replace num with:\n",
    "14. hd (heart disease): categorical, 2 values {0: no, 1: yes}\n",
    "'''\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "# Given two histograms x and y (with the same range and binning), the following function\n",
    "# calculates the intrinsic discrepancy (a symmetrized Kullback-Leibler distance) between them.\n",
    "def intrinsic_discrepancy(x,y):\n",
    "    assert len(x)==len(y)\n",
    "    sumx = sum(xval for xval in x)\n",
    "    sumy = sum(yval for yval in y)\n",
    "    id1  = 0.0\n",
    "    id2  = 0.0\n",
    "    for (xval,yval) in zip(x,y):\n",
    "        if (xval>0) and (yval>0):\n",
    "            id1 += (float(xval)/sumx) * np.log((float(xval)/sumx)/(float(yval)/sumy))\n",
    "            id2 += (float(yval)/sumy) * np.log((float(yval)/sumy)/(float(xval)/sumx))\n",
    "    return min(id1,id2)\n",
    "\n",
    "# Create Pandas dataframe.\n",
    "columns = [\"age\", \"sex\", \"cp\", \"restbp\", \"chol\", \"fbs\", \"restecg\", \n",
    "           \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
    "df0     = pd.read_table(\"data/heart_disease_all14.csv\", sep=',', header=None, names=columns)\n",
    "\n",
    "# Make some plots to check the data\n",
    "fig, axes = plt.subplots( nrows=10, ncols=3, figsize=(15,40) )\n",
    "plt.subplots_adjust( wspace=0.20, hspace=0.20, top=0.97 )\n",
    "plt.suptitle(\"Heart Disease Data\", fontsize=20)\n",
    "axes[0,0].hist(df0.age)\n",
    "axes[0,0].set_xlabel(\"Age (years)\")\n",
    "axes[0,0].set_ylabel(\"Number of Patients\")\n",
    "axes[0,1].hist(df0.sex)\n",
    "axes[0,1].set_xlabel(\"Sex (0=female,1=male)\")\n",
    "axes[0,1].set_ylabel(\"Number of Patients\")\n",
    "axes[0,2].hist(df0.cp,bins=4,range=(0.5,4.5),rwidth=0.80)\n",
    "axes[0,2].set_xlim(0.0,5.0)\n",
    "axes[0,2].set_xlabel(\"Type of Chest Pain [cp]\")\n",
    "axes[0,2].set_ylabel(\"Number of Patients\")\n",
    "axes[1,0].hist(df0.restbp)\n",
    "axes[1,0].set_xlabel(\"Resting Blood Pressure [restbp]\")\n",
    "axes[1,0].set_ylabel(\"Number of Patients\")\n",
    "axes[1,1].hist(df0.chol)\n",
    "axes[1,1].set_xlabel(\"Serum Cholesterol [chol]\")\n",
    "axes[1,1].set_ylabel(\"Number of Patients\")\n",
    "axes[1,2].hist(df0.fbs)\n",
    "axes[1,2].set_xlabel(\"Fasting Blood Sugar [fbs]\")\n",
    "axes[1,2].set_ylabel(\"Number of Patients\")\n",
    "axes[2,0].hist(df0.restecg)\n",
    "axes[2,0].set_xlabel(\"Resting Electrocardiography [restecg]\")\n",
    "axes[2,0].set_ylabel(\"Number of Patients\")\n",
    "axes[2,1].hist(df0.thalach)\n",
    "axes[2,1].set_xlabel(\"Maximum Heart Rate Achieved [thalach]\")\n",
    "axes[2,1].set_ylabel(\"Number of Patients\")\n",
    "axes[2,2].hist(df0.exang)\n",
    "axes[2,2].set_xlabel(\"Exercise Induced Angina [exang]\")\n",
    "axes[2,2].set_ylabel(\"Number of Patients\")\n",
    "axes[3,0].hist(df0.oldpeak)\n",
    "axes[3,0].set_xlabel(\"Exercise Induced ST Depression [oldpeak]\")\n",
    "axes[3,0].set_ylabel(\"Number of Patients\")\n",
    "axes[3,1].hist(df0.slope)\n",
    "axes[3,1].set_xlabel(\"Slope of Peak Exercise ST Segment [slope]\")\n",
    "axes[3,1].set_ylabel(\"Number of Patients\")\n",
    "axes[3,2].hist(df0.ca,bins=4,range=(-0.5,3.5),rwidth=0.8)\n",
    "axes[3,2].set_xlim(-0.7,3.7)\n",
    "axes[3,2].set_xlabel(\"Major Vessels colored by Fluoroscopy [ca]\")\n",
    "axes[3,2].set_ylabel(\"Number of Patients\")\n",
    "axes[4,0].hist(df0.thal)\n",
    "axes[4,0].set_xlabel(\"Thal\")\n",
    "axes[4,0].set_ylabel(\"Number of Patients\")\n",
    "axes[4,1].hist(df0.num,bins=5,range=(-0.5,4.5),rwidth=0.8)\n",
    "axes[4,1].set_xlim(-0.7,4.7)\n",
    "axes[4,1].set_xlabel(\"Heart Disease [num]\")\n",
    "axes[4,1].set_ylabel(\"Number of Patients\")\n",
    "axes[4,2].axis(\"off\")\n",
    "\n",
    "# Marginal feature distributions compared for disease and no-disease (likelihoods)\n",
    "bins = np.linspace(20, 80, 15)\n",
    "axes[5,0].hist(df0[df0.num>0].age.tolist(),bins,color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[5,0].hist(df0[df0.num==0].age,bins,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[5,0].set_xlabel(\"Age (years)\")\n",
    "axes[5,0].set_ylim(0.0,0.070)\n",
    "axes[5,0].legend(prop={'size': 10},loc=\"upper left\")\n",
    "axes[5,1].hist(df0[df0.num>0].sex.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[5,1].hist(df0[df0.num==0].sex,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[5,1].set_xlabel(\"Sex (0=female,1=male)\")\n",
    "axes[5,1].legend(prop={'size': 10},loc=\"upper left\")\n",
    "axes[5,2].hist(df0[df0.num>0].cp.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[5,2].hist(df0[df0.num==0].cp,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[5,2].set_xlabel(\"Type of Chest Pain [cp]\")\n",
    "axes[5,2].legend(prop={'size': 10},loc=\"upper left\")\n",
    "bins = np.linspace(80, 200, 15)\n",
    "axes[6,0].hist(df0[df0.num>0].restbp.tolist(),bins,color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[6,0].hist(df0[df0.num==0].restbp,bins,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[6,0].set_xlabel(\"Resting Blood Pressure [restbp]\")\n",
    "axes[6,0].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[6,1].hist(df0[df0.num>0].chol.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[6,1].hist(df0[df0.num==0].chol,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[6,1].set_xlabel(\"Serum Cholesterol [chol]\")\n",
    "axes[6,1].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[6,2].hist(df0[df0.num>0].fbs.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[6,2].hist(df0[df0.num==0].fbs,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[6,2].set_xlabel(\"Fasting blood sugar [fbs]\")\n",
    "axes[6,2].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[7,0].hist(df0[df0.num>0].restecg.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[7,0].hist(df0[df0.num==0].restecg,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[7,0].set_xlabel(\"Rest ECG\")\n",
    "axes[7,0].set_ylim(0.0,4.0)\n",
    "axes[7,0].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[7,1].hist(df0[df0.num>0].thalach.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[7,1].hist(df0[df0.num==0].thalach,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[7,1].set_xlabel(\"thalach\")\n",
    "axes[7,1].legend(prop={'size': 10},loc=\"upper left\")\n",
    "axes[7,2].hist(df0[df0.num>0].exang.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[7,2].hist(df0[df0.num==0].exang,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[7,2].set_xlabel(\"exang\")\n",
    "axes[7,2].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[8,0].hist(df0[df0.num>0].oldpeak.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[8,0].hist(df0[df0.num==0].oldpeak,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[8,0].set_xlabel(\"oldpeak\")\n",
    "axes[8,0].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[8,1].hist(df0[df0.num>0].slope.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[8,1].hist(df0[df0.num==0].slope,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[8,1].set_xlabel(\"slope\")\n",
    "axes[8,1].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[8,2].hist(df0[df0.num>0].ca.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[8,2].hist(df0[df0.num==0].ca,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[8,2].set_xlabel(\"ca\")\n",
    "axes[8,2].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[9,0].hist(df0[df0.num>0].thal.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[9,0].hist(df0[df0.num==0].thal,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[9,0].set_xlabel(\"thal\")\n",
    "axes[9,0].set_ylim(0.0,2.5)\n",
    "axes[9,0].legend(prop={'size': 10},loc=\"upper right\")\n",
    "axes[9,1].axis(\"off\")\n",
    "axes[9,2].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Compute intrinsic discrepancies between disease and no-disease feature distributions\n",
    "int_discr = {}\n",
    "hist,bin_edges   = np.histogram(df0.age,density=False)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].age,bins=bin_edges,density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].age,bins=bin_edges,density=False)\n",
    "int_discr[\"age\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].sex,bins=(-0.5,0.5,1.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].sex,bins=(-0.5,0.5,1.5),density=False)\n",
    "int_discr[\"sex\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].cp,bins=(0.5,1.5,2.5,3.5,4.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].cp,bins=(0.5,1.5,2.5,3.5,4.5),density=False)\n",
    "int_discr[\"cp\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist,bin_edges   = np.histogram(df0.restbp,density=False)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].restbp,bins=bin_edges,density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].restbp,bins=bin_edges,density=False)\n",
    "int_discr[\"restbp\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist,bin_edges   = np.histogram(df0.chol,density=False)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].chol,bins=bin_edges,density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].chol,bins=bin_edges,density=False)\n",
    "int_discr[\"chol\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].fbs,bins=(-0.5,0.5,1.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].fbs,bins=(-0.5,0.5,1.5),density=False)\n",
    "int_discr[\"fbs\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].restecg,bins=(-0.5,0.5,1.5,2.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].restecg,bins=(-0.5,0.5,1.5,2.5),density=False)\n",
    "int_discr[\"restecg\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist,bin_edges   = np.histogram(df0.thalach,density=False)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].thalach,bins=bin_edges,density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].thalach,bins=bin_edges,density=False)\n",
    "int_discr[\"thalach\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].exang,bins=(-0.5,0.5,1.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].exang,bins=(-0.5,0.5,1.5),density=False)\n",
    "int_discr[\"exang\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist,bin_edges   = np.histogram(df0.oldpeak,density=False)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].oldpeak,bins=bin_edges,density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].oldpeak,bins=bin_edges,density=False)\n",
    "int_discr[\"oldpeak\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].slope,bins=(0.5,1.5,2.5,3.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].slope,bins=(0.5,1.5,2.5,3.5),density=False)\n",
    "int_discr[\"slope\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].ca,bins=(-0.5,0.5,1.5,2.5,3.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].ca,bins=(-0.5,0.5,1.5,2.5,3.5),density=False)\n",
    "int_discr[\"ca\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "hist1,bin_edges1 = np.histogram(df0[df0.num>0].thal,bins=(2.5,3.5,6.5,7.5),density=False)\n",
    "hist2,bin_edges2 = np.histogram(df0[df0.num==0].thal,bins=(2.5,3.5,6.5,7.5),density=False)\n",
    "int_discr[\"thal\"] = intrinsic_discrepancy(hist1,hist2)\n",
    "id_list = Counter(int_discr).most_common()\n",
    "print 'Intrinsic discrepancies between disease and no-disease, in decreasing order: '\n",
    "for item in id_list:\n",
    "    print '   %f  (%s)' % (item[1],item[0])\n",
    "\n",
    "# Convert categorical variables with more than two values into dummy variables.\n",
    "# Note that variable ca is discrete but not categorical, so we don't convert it.\n",
    "df      = df0.copy()\n",
    "dummies = pd.get_dummies(df[\"cp\"],prefix=\"cp\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"cp\"]\n",
    "del df[\"cp_4.0\"]\n",
    "df      = df.rename(columns = {\"cp_1.0\":\"cp_1\",\"cp_2.0\":\"cp_2\",\"cp_3.0\":\"cp_3\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"restecg\"],prefix=\"recg\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"restecg\"]\n",
    "del df[\"recg_0.0\"]\n",
    "df      = df.rename(columns = {\"recg_1.0\":\"recg_1\",\"recg_2.0\":\"recg_2\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"slope\"],prefix=\"slope\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"slope\"]\n",
    "del df[\"slope_2.0\"]\n",
    "df      = df.rename(columns = {\"slope_1.0\":\"slope_1\",\"slope_3.0\":\"slope_3\"})\n",
    "\n",
    "dummies = pd.get_dummies(df[\"thal\"],prefix=\"thal\")\n",
    "df      = df.join(dummies)\n",
    "del df[\"thal\"]\n",
    "del df[\"thal_3.0\"]\n",
    "df      = df.rename(columns = {\"thal_6.0\":\"thal_6\",\"thal_7.0\":\"thal_7\"})\n",
    "\n",
    "# Replace response variable values and rename\n",
    "df[\"num\"].replace(to_replace=[1,2,3,4],value=1,inplace=True)\n",
    "df      = df.rename(columns = {\"num\":\"hd\"})\n",
    "\n",
    "# New list of column labels after the above operations\n",
    "new_columns_1 = [\"age\", \"sex\", \"restbp\", \"chol\", \"fbs\", \"thalach\", \n",
    "                 \"exang\", \"oldpeak\", \"ca\", \"hd\", \"cp_1\", \"cp_2\",\n",
    "                 \"cp_3\", \"recg_1\", \"recg_2\", \"slope_1\", \"slope_3\",\n",
    "                 \"thal_6\", \"thal_7\"]\n",
    "\n",
    "print '\\nNumber of patients in dataframe: %i, with disease: %i, without disease: %i\\n' \\\n",
    "      % (len(df.index),len(df[df.hd==1].index),len(df[df.hd==0].index))\n",
    "print df.head()\n",
    "print df.describe()\n",
    "\n",
    "# Standardize the dataframe\n",
    "stdcols = [\"age\",\"restbp\",\"chol\",\"thalach\",\"oldpeak\"]\n",
    "nrmcols = [\"ca\"]\n",
    "stddf   = df.copy()\n",
    "stddf[stdcols] = stddf[stdcols].apply(lambda x: (x-x.mean())/x.std())\n",
    "stddf[nrmcols] = stddf[nrmcols].apply(lambda x: (x-x.mean())/(x.max()-x.min()))\n",
    "\n",
    "new_columns_2 = new_columns_1[:9] + new_columns_1[10:]\n",
    "new_columns_2.insert(0,new_columns_1[9])\n",
    "stddf = stddf.reindex(columns=new_columns_2)\n",
    "\n",
    "# Convert dataframe into lists for use by classifiers\n",
    "yall = stddf[\"hd\"]\n",
    "Xall = stddf[new_columns_2[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select some clean plots for the blog.\n",
    "'''\n",
    "fig, axes = plt.subplots( nrows=4, ncols=4, figsize=(20,20) )\n",
    "plt.subplots_adjust( wspace=0.20, hspace=0.60, top=0.955)\n",
    "plt.suptitle(\"Heart Disease Data\", fontsize=20)\n",
    "\n",
    "bins = np.linspace(20, 80, 15)\n",
    "axes[0,0].hist(df0[df0.num>0].age.tolist(),bins,color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[0,0].hist(df0[df0.num==0].age,bins,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[0,0].set_xlabel(\"Age (years)\",fontsize=15)\n",
    "axes[0,0].set_ylim(0.0,0.080)\n",
    "axes[0,0].legend(prop={'size': 15},loc=\"upper left\")\n",
    "\n",
    "bins     = np.arange(2)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"sex\"].groupby(df0[\"sex\"]).count()\n",
    "heights2 = df0[df0.num==0][\"sex\"].groupby(df0[\"sex\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[0,1].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[0,1].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[0,1].set_xlabel(\"Sex\",fontsize=15)\n",
    "axes[0,1].set_xticks(bins)\n",
    "axes[0,1].set_xticklabels([\"female\",\"male\"],ha=\"center\")\n",
    "\n",
    "bins     = np.arange(4)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"cp\"].groupby(df0[\"cp\"]).count()\n",
    "heights2 = df0[df0.num==0][\"cp\"].groupby(df0[\"cp\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[0,2].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[0,2].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[0,2].set_xlabel(\"Type of Chest Pain\",fontsize=15)\n",
    "axes[0,2].set_xticks(bins)\n",
    "axes[0,2].set_xticklabels([\"typical angina\", \"atypical angina\", \"non-angina\", \"asymptomatic angina\"],\n",
    "                          ha=\"right\",rotation=45.)\n",
    "\n",
    "bins = np.linspace(80, 200, 15)\n",
    "axes[0,3].hist(df0[df0.num>0].restbp.tolist(),bins,color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[0,3].hist(df0[df0.num==0].restbp,bins,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[0,3].set_xlabel(\"Resting Blood Pressure (mm Hg)\",fontsize=15)\n",
    "\n",
    "axes[1,0].hist(df0[df0.num>0].chol.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[1,0].hist(df0[df0.num==0].chol,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[1,0].set_xlabel(\"Serum Cholesterol (mg/dl)\",fontsize=15)\n",
    "\n",
    "bins     = np.arange(2)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"fbs\"].groupby(df0[\"fbs\"]).count()\n",
    "heights2 = df0[df0.num==0][\"fbs\"].groupby(df0[\"fbs\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[1,1].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=(0.917,0.083,0,0.75),color=[\"none\"],label=\"disease\")\n",
    "axes[1,1].bar(bins,heights2,width,align=\"center\",edgecolor=(0.467,0.533,0,0.75),color=[\"none\"],label=\"no disease\")\n",
    "axes[1,1].set_xlabel(\"Fasting Blood Sugar\",fontsize=15)\n",
    "axes[1,1].set_xticks(bins)\n",
    "axes[1,1].set_xticklabels([\"< 120 mg/dl\",\"> 120 mg/dl\"],ha=\"center\")\n",
    "\n",
    "bins     = np.arange(3)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"restecg\"].groupby(df0[\"restecg\"]).count()\n",
    "heights2 = df0[df0.num==0][\"restecg\"].groupby(df0[\"restecg\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[1,2].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[1,2].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[1,2].set_xlabel(\"Rest ECG\",fontsize=15)\n",
    "axes[1,2].set_xticks(bins)\n",
    "axes[1,2].set_xticklabels([\"Normal\",\"ST-T wave abnorm.\",\"left ventr. hypertrophy\"],ha=\"right\",rotation=45.)\n",
    "\n",
    "axes[1,3].hist(df0[df0.num>0].thalach.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[1,3].hist(df0[df0.num==0].thalach,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[1,3].set_xlabel(\"Thalium Test: Max. Heart Rate\",fontsize=15)\n",
    "\n",
    "bins     = np.arange(2)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"exang\"].groupby(df0[\"exang\"]).count()\n",
    "heights2 = df0[df0.num==0][\"exang\"].groupby(df0[\"exang\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[2,0].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[2,0].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[2,0].set_xlabel(\"Exercise Induced Angina\",fontsize=15)\n",
    "axes[2,0].set_xticks(bins)\n",
    "axes[2,0].set_xticklabels([\"No\",\"Yes\"],ha=\"center\")\n",
    "\n",
    "axes[2,1].hist(df0[df0.num>0].oldpeak.tolist(),color=[\"crimson\"],histtype=\"step\",label=\"disease\",normed=True)\n",
    "axes[2,1].hist(df0[df0.num==0].oldpeak,color=[\"chartreuse\"],histtype=\"step\",label=\"no disease\",normed=True)\n",
    "axes[2,1].set_xlabel(\"ST Depression Induced by Exercise\", fontsize=15)\n",
    "\n",
    "bins     = np.arange(3)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"slope\"].groupby(df0[\"slope\"]).count()\n",
    "heights2 = df0[df0.num==0][\"slope\"].groupby(df0[\"slope\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[2,2].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[2,2].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[2,2].set_xlabel(\"Slope of Peak Exercise ST Segment\",fontsize=15)\n",
    "axes[2,2].set_xticks(bins)\n",
    "axes[2,2].set_xticklabels([\"Upsloping\",\"Flat\",\"Downsloping\"],ha=\"right\",rotation=45.)\n",
    "\n",
    "bins     = np.arange(4)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"ca\"].groupby(df0[\"ca\"]).count()\n",
    "heights2 = df0[df0.num==0][\"ca\"].groupby(df0[\"ca\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[2,3].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[2,3].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[2,3].set_xlabel(\"Major Vessels Colored by Fluoroscopy\",fontsize=15)\n",
    "axes[2,3].set_xticks(bins)\n",
    "axes[2,3].set_xticklabels([\"0\",\"1\",\"2\",\"3\"],ha=\"center\")\n",
    "\n",
    "bins     = np.arange(3)\n",
    "width    = 0.5\n",
    "heights1 = df0[df0.num>0][\"thal\"].groupby(df0[\"thal\"]).count()\n",
    "heights2 = df0[df0.num==0][\"thal\"].groupby(df0[\"thal\"]).count()\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[3,0].bar(bins+0.025,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[3,0].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[3,0].set_xlabel(\"Thalium Stress Test Result\",fontsize=15)\n",
    "axes[3,0].set_xticks(bins)\n",
    "axes[3,0].set_xticklabels([\"Normal\",\"Fixed Defect\",\"Reversible Defect\"],ha=\"right\",rotation=45.)\n",
    "axes[3,0].set_ylim(0.0,1.0)\n",
    "\n",
    "bins     = np.arange(5)\n",
    "width    = 0.5\n",
    "heights1 = np.array(map(float,[0]+df0[df0.num>0][\"num\"].groupby(df0[\"num\"]).count().tolist()))\n",
    "heights2 = np.array(map(float,df0[df0.num==0][\"num\"].groupby(df0[\"num\"]).count().tolist()+[0,0,0,0]))\n",
    "heights1 = heights1/sum(heights1)\n",
    "heights2 = heights2/sum(heights2)\n",
    "axes[3,1].bar(bins,heights1,width,align=\"center\",edgecolor=[\"crimson\"],color=[\"none\"],label=\"disease\")\n",
    "axes[3,1].bar(bins,heights2,width,align=\"center\",edgecolor=[\"chartreuse\"],color=[\"none\"],label=\"no disease\")\n",
    "axes[3,1].set_xlabel(\"Major Vessels with >50% Narrowing\",fontsize=15)\n",
    "axes[3,1].set_xticks(bins)\n",
    "axes[3,1].set_xticklabels([\"0\",\"1\",\"2\",\"3\",\"4\"],ha=\"center\")\n",
    "axes[3,1].set_ylim(0.0,1.1)\n",
    "\n",
    "axes[3,2].axis(\"off\")\n",
    "axes[3,3].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(\"mcnulty_fig1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note about logistic regression with scikit-learn and with statsmodels:\n",
    "* In scikit-learn we can specify penalty=\"l1\" or penalty=\"l2\", with\n",
    "  an associated C=c, which is the *inverse* of the regularization strength.\n",
    "  Thus, for zero regularization specify a high value of c.  Scikit-learn\n",
    "  does not calculate uncertainties on the fit coefficients.\n",
    "* In statsmodels we can specify method=\"l1\" and an associated regularization\n",
    "  strength alpha.  There is no method=\"l2\", but no-regularization can be\n",
    "  obtained by setting alpha=0.  Statsmodels does compute uncertainties on\n",
    "  the fit coefficients.\n",
    "In the following couple of cells we do a fit to the heart-disease data\n",
    "with all features included, first with scikit-learn, then with statsmodels.\n",
    "Let's make sure we understand what we are doing by obtaining the same results\n",
    "with both software packages.\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "lasso = False\n",
    "\n",
    "nfeatures = len(stddf.columns)\n",
    "if lasso:           # lasso regularization\n",
    "    penalty = \"l1\"\n",
    "    cval    = 1.0\n",
    "    alpha   = [1.0]*nfeatures\n",
    "else:               # no regularization\n",
    "    penalty = \"l1\"\n",
    "    cval    = 1000.0\n",
    "    alpha   = 0.0\n",
    "\n",
    "model = LogisticRegression(fit_intercept=True,penalty=penalty,dual=False,C=cval)\n",
    "print(model)\n",
    "lrfit = model.fit(Xall,yall)\n",
    "print('\\nLogisticRegression score on full data set: %f\\n' % lrfit.score(Xall,yall))\n",
    "ypred = model.predict(Xall)\n",
    "print '\\nClassification report on full data set:'\n",
    "print(metrics.classification_report(yall,ypred))\n",
    "print '\\nConfusion matrix:'\n",
    "print(metrics.confusion_matrix(yall,ypred))\n",
    "print '\\nLogisticRegression coefficients:'\n",
    "coeff = model.coef_.tolist()[0]\n",
    "for index in range(len(coeff)):\n",
    "    print '%s : %8.5f' % (new_columns_2[index+1].rjust(9),coeff[index])\n",
    "print 'Intercept : %f' %model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now with statsmodels\n",
    "'''\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df1              = stddf.copy()\n",
    "df1[\"intercept\"] = 1.0\n",
    "\n",
    "train_cols = df1[df1.columns[1:]]\n",
    "response   = df1[df1.columns[0]]\n",
    "model      = sm.Logit(response, train_cols, missing=\"raise\")\n",
    " \n",
    "# fit the model\n",
    "print('\\nNumber of features in model: %i' %nfeatures)\n",
    "print('Lasso = %s' % lasso)\n",
    "if lasso: print('alpha=%s' %alpha)\n",
    "print(\" \")\n",
    "\n",
    "if lasso:\n",
    "    result = model.fit_regularized(method=penalty, alpha=alpha, disp=1)\n",
    "else:\n",
    "    result = model.fit(method=\"newton\", maxiter=100, disp=1)\n",
    "\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The previous two cells demonstrate that we can obtain the same results\n",
    "with scikit-learn and statsmodels.  However, the results show a problem:\n",
    "the fit coefficient of the \"age\" feature is negative, indicating that \n",
    "the risk of heart disease might decrease with age!  This must be the \n",
    "result of a correlation effect, because when we fit age alone the coefficient\n",
    "is positive (see also the plot of the marginal distributions of age at the\n",
    "beginning).  To see where the correlation comes from, we are going to repeat\n",
    "the fit by adding one feature at a time to the age feature, until the sign\n",
    "of the age fit coefficient flips from + to -.\n",
    "'''\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "\n",
    "print('\\nNumber of Features = %i, Lasso = %s\\n' %(nfeatures,lasso))\n",
    "iterable   = range(2,nfeatures)\n",
    "for s in xrange(len(iterable)+1,0,-1):\n",
    "    for comb in itertools.combinations(iterable, s):\n",
    "        dropped_columns  = list(comb)\n",
    "        df1              = stddf.copy()\n",
    "        df1              = df1.drop(df1.columns[dropped_columns],axis=1) \n",
    "#        df1[\"intercept\"] = 1.0\n",
    "        train_cols       = df1[df1.columns[1:]]\n",
    "        response         = df1[df1.columns[0]]\n",
    "        model            = sm.Logit(response, train_cols)\n",
    "        if lasso:\n",
    "            result = model.fit_regularized(method=penalty, alpha=alpha, disp=0)\n",
    "        else:\n",
    "            result = model.fit(method=\"newton\", maxiter=100, disp=0)\n",
    "        print 'With the columns %s, \"age\" coefficient = %f +/- %f' \\\n",
    "        % (list(df1.columns[1:].values),result.params[0],result.bse[0])\n",
    "        if result.params[0] < 0.0:\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions:\n",
    "\n",
    "* Fitting age alone gives a positive coefficient, as expected.\n",
    "* It takes at least two additional features to cause the age coefficient to turn negative (for example \"thalach\" and \"ca\").  However the negative age coefficient is not significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make correlation plots for the features age, thalach, and ca.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots( nrows=3, ncols=3, figsize=(15,15) )\n",
    "plt.suptitle(\"Correlations\", fontsize=20)\n",
    "axes[0,0].scatter(df[df.hd==1].age,df[df.hd==1].thalach,color=[\"crimson\"],label=\"disease\")\n",
    "axes[0,0].scatter(df[df.hd==0].age,df[df.hd==0].thalach,color=[\"chartreuse\"],label=\"no disease\")\n",
    "axes[0,0].set_xlabel(\"Age (years)\")\n",
    "axes[0,0].set_ylabel(\"thalach\")\n",
    "axes[0,0].legend(prop={'size': 10},loc=\"lower left\")\n",
    "axes[0,1].scatter(df[df.hd==1].age,df[df.hd==1].ca,color=[\"crimson\"],s=80,alpha=0.5,label=\"disease\")\n",
    "axes[0,1].scatter(df[df.hd==0].age,df[df.hd==0].ca,color=[\"chartreuse\"],s=15,label=\"no disease\")\n",
    "axes[0,1].set_xlabel(\"Age (years)\")\n",
    "axes[0,1].set_ylabel(\"ca\")\n",
    "axes[0,2].scatter(df[df.hd==1].thalach,df[df.hd==1].ca,color=[\"crimson\"],s=80,alpha=0.5,label=\"disease\")\n",
    "axes[0,2].scatter(df[df.hd==0].thalach,df[df.hd==0].ca,color=[\"chartreuse\"],s=15,label=\"no disease\")\n",
    "axes[0,2].set_xlabel(\"thalach\")\n",
    "axes[0,2].set_ylabel(\"ca\")\n",
    "axes[1,0].scatter(df[(df.hd==1)&(df.ca==0)].age,df[(df.hd==1)&(df.ca==0)].thalach,\\\n",
    "                  color=[\"crimson\"],label=\"disease\")\n",
    "axes[1,0].scatter(df[(df.hd==0)&(df.ca==0)].age,df[(df.hd==0)&(df.ca==0)].thalach,\\\n",
    "                  color=[\"chartreuse\"],label=\"no disease\")\n",
    "axes[1,0].text(22.0,212.0,\"ca=0\")\n",
    "axes[1,0].set_xlabel(\"Age (years)\")\n",
    "axes[1,0].set_ylabel(\"thalach\")\n",
    "bins = np.linspace(20, 80, 15)\n",
    "axes[1,1].hist(df[(df.hd==1)&(df.ca==0)].age.tolist(),bins,color=[\"crimson\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"disease\")\n",
    "axes[1,1].hist(df[(df.hd==0)&(df.ca==0)].age.tolist(),bins,color=[\"chartreuse\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"no disease\")\n",
    "axes[1,1].text(22.0,0.047,\"ca=0\")\n",
    "axes[1,1].set_xlabel(\"Age (years)\")\n",
    "bins = np.linspace(60, 220, 15)\n",
    "axes[1,2].hist(df[(df.hd==1)&(df.ca==0)].thalach.tolist(),bins,color=[\"crimson\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"disease\")\n",
    "axes[1,2].hist(df[(df.hd==0)&(df.ca==0)].thalach.tolist(),bins,color=[\"chartreuse\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"no disease\")\n",
    "axes[1,2].text(64.0,0.0235,\"ca=0\")\n",
    "axes[1,2].set_xlabel(\"thalach\")\n",
    "axes[2,0].scatter(df[(df.hd==1)&(df.ca>0)].age,df[(df.hd==1)&(df.ca>0)].thalach,\\\n",
    "                  color=[\"crimson\"],label=\"disease\")\n",
    "axes[2,0].scatter(df[(df.hd==0)&(df.ca>0)].age,df[(df.hd==0)&(df.ca>0)].thalach,\\\n",
    "                  color=[\"chartreuse\"],label=\"no disease\")\n",
    "axes[2,0].text(37.0,212.0,\"ca>0\")\n",
    "axes[2,0].set_xlabel(\"Age (years)\")\n",
    "axes[2,0].set_ylabel(\"thalach\")\n",
    "bins = np.linspace(20, 80, 8)\n",
    "axes[2,1].hist(df[(df.hd==1)&(df.ca>0)].age.tolist(),bins,color=[\"crimson\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"disease\")\n",
    "axes[2,1].hist(df[(df.hd==0)&(df.ca>0)].age.tolist(),bins,color=[\"chartreuse\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"no disease\")\n",
    "axes[2,1].text(22.0,0.065,\"ca>0\")\n",
    "axes[2,1].set_xlabel(\"Age (years)\")\n",
    "axes[2,1].set_ylim(0.0,0.07)\n",
    "bins = np.linspace(60, 220, 8)\n",
    "axes[2,2].hist(df[(df.hd==1)&(df.ca>0)].thalach.tolist(),bins,color=[\"crimson\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"disease\")\n",
    "axes[2,2].hist(df[(df.hd==0)&(df.ca>0)].thalach.tolist(),bins,color=[\"chartreuse\"],\\\n",
    "               histtype=\"step\",normed=True,label=\"no disease\")\n",
    "axes[2,2].text(64.0,0.0235,\"ca>0\")\n",
    "axes[2,2].set_xlabel(\"thalach\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Fit using only the age and thalach features, separately for ca=0 and ca>0.\n",
    "'''\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "dropped_columns  = [2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18] # Drop all features except 1 (age) and 6 (thalach).\n",
    "\n",
    "# Fit for ca=0 first (ca<0 is equivalent):\n",
    "df1        = stddf.copy()\n",
    "df1        = df1[df1.ca<0]\n",
    "df1        = df1.drop(df1.columns[dropped_columns],axis=1) \n",
    "train_cols = df1[df1.columns[1:]]\n",
    "response   = df1[df1.columns[0]]\n",
    "model      = sm.Logit(response, train_cols)\n",
    "if lasso:\n",
    "    result = model.fit_regularized(method=penalty, alpha=alpha, disp=0)\n",
    "else:\n",
    "    result = model.fit(method=\"newton\", maxiter=100, disp=0)\n",
    "print('\\nFit results with ca<0 and Lasso = %s\\n' %lasso)\n",
    "print result.summary()\n",
    "\n",
    "# Now fit for ca>0:\n",
    "df1        = stddf.copy()\n",
    "df1        = df1[df1.ca>0]\n",
    "df1        = df1.drop(df1.columns[dropped_columns],axis=1) \n",
    "train_cols = df1[df1.columns[1:]]\n",
    "response   = df1[df1.columns[0]]\n",
    "model      = sm.Logit(response, train_cols)\n",
    "if lasso:\n",
    "    result = model.fit_regularized(method=penalty, alpha=alpha, disp=0)\n",
    "else:\n",
    "    result = model.fit(method=\"newton\", maxiter=100, disp=0)\n",
    "print('\\nFit results with ca>0 and Lasso = %s\\n' %lasso)\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions:\n",
    "* For ca=0 the age coefficient is negative but not significant.\n",
    "* For ca>0 the age coefficient is positive and almost significant.\n",
    "* In both cases the thalach coefficient is negative and significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select features to keep: Method 1\n",
    "Optimize logistic regression on accuracy score, using cross-validation.\n",
    "Here we use sklearn since it includes a cross_validation method.\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "best_score = []\n",
    "best_std   = []\n",
    "best_comb  = []\n",
    "nfeatures  = 18\n",
    "iterable   = range(nfeatures)\n",
    "for intercept in [True,False]:\n",
    "    model  = LogisticRegression(fit_intercept=intercept,penalty=penalty,dual=False,C=cval)\n",
    "    for s in xrange(len(iterable)+1):\n",
    "        for comb in itertools.combinations(iterable, s):\n",
    "            if len(comb) > 0:\n",
    "                Xsel = []\n",
    "                for patient in Xall:\n",
    "                    Xsel.append([patient[ind] for ind in comb])\n",
    "                this_scores = cross_val_score(model, Xsel, y=yall, cv=3 )\n",
    "                score_mean  = np.mean(this_scores)\n",
    "                score_std   = np.std(this_scores)\n",
    "                comb1       = list(comb)\n",
    "                if intercept: comb1.append(nfeatures)\n",
    "                if len(best_score) > 0: \n",
    "                    if score_mean > best_score[0]:\n",
    "                        best_score = []\n",
    "                        best_std   = []\n",
    "                        best_comb  = []\n",
    "                        best_score.append(score_mean)\n",
    "                        best_std.append(score_std)\n",
    "                        best_comb.append(comb1)\n",
    "                    elif score_mean == best_score[0]:\n",
    "                        best_score.append(score_mean)\n",
    "                        best_std.append(score_std)\n",
    "                        best_comb.append(comb1)\n",
    "                else:\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb1)\n",
    "\n",
    "new_columns_3 = new_columns_2[1:]\n",
    "new_columns_3.append(\"intercept\")\n",
    "num_ties = len(best_score)\n",
    "for ind in range(num_ties):\n",
    "    comb1 = best_comb[ind][:]\n",
    "    if nfeatures in comb1:\n",
    "        intercept = True\n",
    "        comb1.remove(nfeatures)\n",
    "    else:\n",
    "        intercept = False\n",
    "    model = LogisticRegression(fit_intercept=intercept,penalty=penalty,dual=False,C=cval)\n",
    "    Xsel  = []\n",
    "    for patient in Xall:\n",
    "        Xsel.append([patient[i] for i in comb1])\n",
    "    lrfit = model.fit(Xsel,yall)\n",
    "    print('\\nResults for combination %s:' %best_comb[ind])\n",
    "    print('LogisticRegression score on full data set:      %f' % lrfit.score(Xsel,yall))\n",
    "    print('LogisticRegression score from cross-validation: %f +/- %f' % (best_score[ind],best_std[ind]))\n",
    "    print('LogisticRegression coefficients:')\n",
    "    coeff = model.coef_.tolist()[0]\n",
    "    if intercept: coeff.append(model.intercept_)\n",
    "    for jnd in range(len(coeff)):\n",
    "        print '%s : %8.5f' % (new_columns_3[best_comb[ind][jnd]].rjust(9),coeff[jnd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age coefficient is still negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select features to keep: Method 2\n",
    "Eliminate features with non-significant coefficients one-by-one, while keeping the model \n",
    "deviance as low as possible.  A coefficient is deemed significant if its fitted z-score \n",
    "is above 2.0.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def select_features(df0,**kwargs):\n",
    "    \"\"\"Selects features for a logistic regression model. Starts by including all features, and then\n",
    "    eliminates non-significant features one-by-one, in such a way as to minimize the increase in \n",
    "    deviance after each elimination.\"\"\"\n",
    "    if \"verbose\" in kwargs: \n",
    "        vb = kwargs[\"verbose\"]\n",
    "    else:\n",
    "        vb = 0\n",
    "    if \"method\" in kwargs:\n",
    "        md = kwargs[\"method\"]\n",
    "    else:\n",
    "        md = \"newton\"\n",
    "    feature_names   = list(df0.columns)      # Get data frame column names, these are the features to be fitted.\n",
    "    feature_names.append(\"intercept\")        # Add an intercept feature.\n",
    "    nfeatures       = len(feature_names) - 1 # First feature is response variable, so doesn't count.\n",
    "    saved_columns   = range(1,nfeatures+1)\n",
    "    dropped_columns = []\n",
    "    while len(saved_columns)>1:\n",
    "        df1              = df0.copy()\n",
    "        df1[\"intercept\"] = 1.0\n",
    "        df1              = df1.drop(df1.columns[dropped_columns],axis=1)\n",
    "        features         = df1[df1.columns[1:]]\n",
    "        response         = df1[df1.columns[0]]\n",
    "        model            = sm.Logit(response, features, missing=\"raise\")\n",
    "        result           = model.fit(method=md, maxiter=100, disp=0)\n",
    "        if len(saved_columns)==nfeatures:\n",
    "            if vb >= 1:\n",
    "                print '\\nInitial -log(L)=%8.5f; fit method = %s\\n' %(-result.llf,md)\n",
    "        if all(np.abs(tval) >= 2.0 for tval in result.tvalues):\n",
    "            if vb >= 1:\n",
    "                print '\\nFinal selection includes %i features:\\n' % len(saved_columns)\n",
    "                print result.summary()\n",
    "            break\n",
    "        else:\n",
    "            llfmin = 999999.0\n",
    "            colmin = -100\n",
    "            for col in saved_columns:\n",
    "                dropped_columns.append(col)\n",
    "                df1              = df0.copy()\n",
    "                df1[\"intercept\"] = 1.0\n",
    "                df1              = df1.drop(df1.columns[dropped_columns],axis=1)\n",
    "                features         = df1[df1.columns[1:]]\n",
    "                response         = df1[df1.columns[0]]\n",
    "                model            = sm.Logit(response, features, missing=\"raise\")\n",
    "                result           = model.fit(method=md, maxiter=100, disp=0)\n",
    "                if -result.llf < llfmin:\n",
    "                    llfmin = -result.llf\n",
    "                    colmin = col\n",
    "                dropped_columns.remove(col)\n",
    "            if vb >= 2:\n",
    "                print 'Dropping %s, -log(L)=%8.5f...' % (feature_names[colmin].rjust(9),llfmin)\n",
    "            saved_columns.remove(colmin)\n",
    "            dropped_columns.append(colmin)\n",
    "    else:\n",
    "        if vb >= 1:\n",
    "            print 'Final selection has no features left'\n",
    "    return result,saved_columns\n",
    "\n",
    "select_features(stddf,verbose=2,method=\"newton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Print out some features of the optimal logistic regression model\n",
    "for the full data set (accuracy, precision, recall,...)\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression(fit_intercept=False,penalty=\"l1\",dual=False,C=1000.0)\n",
    "comb = [1,2,5,8,9,10,11,14,17]\n",
    "print comb\n",
    "Xsel  = []\n",
    "for patient in Xall:\n",
    "    Xsel.append([patient[ind] for ind in comb])\n",
    "print(model)\n",
    "lrfit = model.fit(Xsel,yall)\n",
    "print('\\nLogisticRegression score on full data set: %f\\n' % lrfit.score(Xsel,yall))\n",
    "ypred = model.predict(Xsel)\n",
    "print '\\nClassification report on full data set:'\n",
    "print(metrics.classification_report(yall,ypred))\n",
    "print '\\nConfusion matrix:'\n",
    "print(metrics.confusion_matrix(yall,ypred))\n",
    "log_reg_coef = model.coef_[0]\n",
    "log_reg_intc = model.intercept_\n",
    "print('\\nLogisticRegression coefficients: %s' %log_reg_coef)\n",
    "print('LogisticRegression intercept: %f' %log_reg_intc)\n",
    "\n",
    "# Separate disease from no-disease cases for a histogram of logistic regression probabilities:\n",
    "Xsel0 = [patient for (patient,status) in zip(Xsel,yall) if status==0] # No-disease cases\n",
    "Xsel1 = [patient for (patient,status) in zip(Xsel,yall) if status==1] # Disease cases\n",
    "print('\\nNumber of disease cases: %i, no-disease cases: %i' %(len(Xsel1),len(Xsel0)))\n",
    "Xsel0_Prob = [p1 for (p0,p1) in model.predict_proba(Xsel0)] # Predicted prob. of heart disease for no-disease cases\n",
    "Xsel1_Prob = [p1 for (p0,p1) in model.predict_proba(Xsel1)] # Predicted prob. of heart disease for disease cases\n",
    "\n",
    "# Here we do a little test to make sure we understand how the logistic regression coefficients work:\n",
    "qsum = 0.0\n",
    "for patient,prob in zip(Xsel0,Xsel0_Prob):\n",
    "    lrprob = 1.0/(1.0+np.exp(-sum(patient[i]*log_reg_coef[i] for i in range(9))-log_reg_intc))\n",
    "    qsum  += (lrprob-prob)**2\n",
    "for patient,prob in zip(Xsel1,Xsel1_Prob):\n",
    "    lrprob = 1.0/(1.0+np.exp(-sum(patient[i]*log_reg_coef[i] for i in range(9))-log_reg_intc))\n",
    "    qsum  += (lrprob-prob)**2\n",
    "print('Sum of quadratic differences between probability calculations: %f' %qsum) # Should be zero!\n",
    "\n",
    "# Here is the plot:\n",
    "fig, axes = plt.subplots( nrows=1, ncols=1, figsize=(6,6) )\n",
    "plt.subplots_adjust( top=0.92 )\n",
    "plt.suptitle(\"Cleveland Data Set\", fontsize=20)\n",
    "axes.hist(Xsel0_Prob,color=[\"chartreuse\"],histtype=\"step\",label=\"no-disease cases (160)\")\n",
    "axes.hist(Xsel1_Prob,color=[\"crimson\"],histtype=\"step\",label=\"disease cases (139)\")\n",
    "axes.set_xlabel(\"Predicted Probability of Disease\",fontsize=15)\n",
    "axes.set_ylabel(\"Number of Patients\",fontsize=15)\n",
    "axes.set_ylim( 0.0, 90.0 )\n",
    "axes.legend(prop={'size': 15},loc=\"upper right\")\n",
    "plt.show()\n",
    "#fig.savefig('LogRegProbabilities.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get unbiased estimates of accuracy, precision and recall by doing cross-validation \n",
    "on a three-way partition of the original data set.  The cross-validation includes\n",
    "both the feature selection procedure and the logistic regression fit.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Make a list of the feature names\n",
    "feature_names = list(stddf.columns)\n",
    "feature_names.append(\"intercept\")\n",
    "\n",
    "# Split standardized Cleveland data set into three (approximately) equal test sets dfp1c, dfp2c, dfp3c.\n",
    "# Then arrange for dfp1, dfp2 and dfp3 to be the complements of dfp1c, dfp2c and dfp3c respectively.\n",
    "dfp1,  dfp1c = train_test_split( stddf, test_size=0.33, random_state=1 ) # dfp1 is 2/3, dfp1c is 1/3 of stddf\n",
    "dfp2c, dfp3c = train_test_split( dfp1,  test_size=0.50, random_state=1 ) # dfp2c, dfp3c are both 1/3 of stddf\n",
    "dfp2 = pd.concat([dfp1c,dfp3c])\n",
    "dfp3 = pd.concat([dfp1c,dfp2c])\n",
    "\n",
    "# Save the response variable from the test data frames.\n",
    "dfp1c_true   = list(dfp1c.hd)\n",
    "dfp2c_true   = list(dfp2c.hd)\n",
    "dfp3c_true   = list(dfp3c.hd)\n",
    "\n",
    "# For each training set, find the optimal set of features.\n",
    "result1,sel1 = select_features(dfp1,verbose=0,method=\"lbfgs\")\n",
    "result2,sel2 = select_features(dfp2,verbose=0,method=\"lbfgs\")\n",
    "result3,sel3 = select_features(dfp3,verbose=0,method=\"lbfgs\")\n",
    "\n",
    "print('Partition 1 features: %s' %[feature_names[ind] for ind in sel1])\n",
    "print('Partition 2 features: %s' %[feature_names[ind] for ind in sel2])\n",
    "print('Partition 3 features: %s' %[feature_names[ind] for ind in sel3])\n",
    "\n",
    "# Drop the non-selected columns from the test data frames.  This is different for 1, 2 and 3.\n",
    "nfeatures = len(stddf.columns)\n",
    "drop1     = [icol for icol in range(nfeatures) if icol not in sel1]\n",
    "drop2     = [icol for icol in range(nfeatures) if icol not in sel2]\n",
    "drop3     = [icol for icol in range(nfeatures) if icol not in sel3]\n",
    "dfp1c     = dfp1c.drop(dfp1c.columns[drop1],axis=1)\n",
    "dfp2c     = dfp2c.drop(dfp2c.columns[drop2],axis=1)\n",
    "dfp3c     = dfp3c.drop(dfp3c.columns[drop3],axis=1)\n",
    "\n",
    "# Intercept is not part of the original dataframes, so it needs to be added explicitly if selected.\n",
    "if 19 in sel1: dfp1c[\"intercept\"] = 1.0\n",
    "if 19 in sel2: dfp2c[\"intercept\"] = 1.0\n",
    "if 19 in sel3: dfp3c[\"intercept\"] = 1.0\n",
    "    \n",
    "# Compute the predictions for the test data sets.\n",
    "hd1_pred  = result1.predict(dfp1c,linear=False)\n",
    "hd2_pred  = result2.predict(dfp2c,linear=False)\n",
    "hd3_pred  = result3.predict(dfp3c,linear=False)\n",
    "\n",
    "hd1_all   = zip(dfp1c_true,hd1_pred)\n",
    "hd2_all   = zip(dfp2c_true,hd2_pred)\n",
    "hd3_all   = zip(dfp3c_true,hd3_pred)\n",
    "\n",
    "# Compute the confusion matrices.\n",
    "nparts = 3\n",
    "yy = range(nparts)\n",
    "yn = range(nparts)\n",
    "nn = range(nparts)\n",
    "ny = range(nparts)\n",
    "for ind,hd_all in enumerate([hd1_all,hd2_all,hd3_all]):\n",
    "    yy[ind] = sum([1 for [tr,pr] in hd_all if tr==1 and pr>=0.5])\n",
    "    yn[ind] = sum([1 for [tr,pr] in hd_all if tr==1 and pr<0.5])\n",
    "    nn[ind] = sum([1 for [tr,pr] in hd_all if tr==0 and pr<0.5])\n",
    "    ny[ind] = sum([1 for [tr,pr] in hd_all if tr==0 and pr>=0.5])\n",
    "    \n",
    "# Compute accuracy, precision, recall\n",
    "acc = range(nparts)\n",
    "pr1 = range(nparts)\n",
    "pr2 = range(nparts)\n",
    "rl1 = range(nparts)\n",
    "rl2 = range(nparts)\n",
    "for ind in range(3):\n",
    "    acc[ind] = float(yy[ind]+nn[ind])/(yy[ind]+yn[ind]+ny[ind]+nn[ind]) # fraction of predictions that are correct\n",
    "    pr1[ind] = float(yy[ind])/(yy[ind]+ny[ind])            # fraction of disease predictions that are correct\n",
    "    pr2[ind] = float(nn[ind])/(nn[ind]+yn[ind])            # fraction of no-disease predictions that are correct\n",
    "    rl1[ind] = float(yy[ind])/(yy[ind]+yn[ind])            # fraction of disease cases that are identified\n",
    "    rl2[ind] = float(nn[ind])/(nn[ind]+ny[ind])            # fraction of no-disease cases that are identified\n",
    "\n",
    "# Compute averages and standard deviations\n",
    "acc_mean = np.mean(acc)\n",
    "acc_err  = np.std(acc,ddof=1)/np.sqrt(nparts)\n",
    "pr1_mean = np.mean(pr1)\n",
    "pr1_err  = np.std(pr1,ddof=1)/np.sqrt(nparts)\n",
    "pr2_mean = np.mean(pr2)\n",
    "pr2_err  = np.std(pr2,ddof=1)/np.sqrt(nparts)\n",
    "rl1_mean = np.mean(rl1)\n",
    "rl1_err  = np.std(rl1,ddof=1)/np.sqrt(nparts)\n",
    "rl2_mean = np.mean(rl2)\n",
    "rl2_err  = np.std(rl2,ddof=1)/np.sqrt(nparts)\n",
    "\n",
    "# Print out results\n",
    "print(' ')\n",
    "print('Accuracy:                [%5.3f, %5.3f, %5.3f]; average= %5.3f +/- %5.3f' \n",
    "      %(acc[0],acc[1],acc[2],acc_mean,acc_err))\n",
    "print('Precision on disease:    [%5.3f, %5.3f, %5.3f]; average= %5.3f +/- %5.3f' \n",
    "      %(pr1[0],pr1[1],pr1[2],pr1_mean,pr1_err))\n",
    "print('Precision on no-disease: [%5.3f, %5.3f, %5.3f]; average= %5.3f +/- %5.3f' \n",
    "      %(pr2[0],pr2[1],pr2[2],pr2_mean,pr2_err))\n",
    "print('Recall on disease:       [%5.3f, %5.3f, %5.3f]; average= %5.3f +/- %5.3f' \n",
    "      %(rl1[0],rl1[1],rl1[2],rl1_mean,rl1_err))\n",
    "print('Recall on no-disease:    [%5.3f, %5.3f, %5.3f]; average= %5.3f +/- %5.3f' \n",
    "      %(rl2[0],rl2[1],rl2[2],rl2_mean,rl2_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimize Gaussian Naive Bayes\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "best_score = []\n",
    "best_std   = []\n",
    "best_comb  = []\n",
    "nfeatures  = 18\n",
    "iterable   = range(nfeatures)\n",
    "gnb        = GaussianNB()\n",
    "for s in xrange(len(iterable)+1):\n",
    "    for comb in itertools.combinations(iterable, s):\n",
    "        if len(comb) > 0:\n",
    "            Xsel = []\n",
    "            for patient in Xall:\n",
    "                Xsel.append([patient[ind] for ind in comb])\n",
    "            this_scores = cross_val_score(gnb, Xsel, y=yall, cv=3 )\n",
    "            score_mean  = np.mean(this_scores)\n",
    "            score_std   = np.std(this_scores)\n",
    "            if len(best_score) > 0: \n",
    "                if score_mean > best_score[0]:\n",
    "                    best_score = []\n",
    "                    best_std   = []\n",
    "                    best_comb  = []\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "                elif score_mean == best_score[0]:\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "            else:\n",
    "                best_score.append(score_mean)\n",
    "                best_std.append(score_std)\n",
    "                best_comb.append(comb)\n",
    "num_ties = len(best_score)\n",
    "for ind in range(num_ties):\n",
    "    print 'For comb=%s, Gaussian Naive Bayes Accuracy = %f +/- %f' \\\n",
    "            % (best_comb[ind],best_score[ind],best_std[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimize support vector classifier\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "best_score = []\n",
    "best_std   = []\n",
    "best_comb  = []\n",
    "nfeatures  = 18\n",
    "iterable   = range(nfeatures)\n",
    "model      = SVC()\n",
    "for s in xrange(len(iterable)+1):\n",
    "    for comb in itertools.combinations(iterable, s):\n",
    "        if len(comb) > 0:\n",
    "            Xsel = []\n",
    "            for patient in Xall:\n",
    "                Xsel.append([patient[ind] for ind in comb])\n",
    "            this_scores = cross_val_score(model, Xsel, y=yall, cv=3 )\n",
    "            score_mean  = np.mean(this_scores)\n",
    "            score_std   = np.std(this_scores)\n",
    "            if len(best_score) > 0: \n",
    "                if score_mean > best_score[0]:\n",
    "                    best_score = []\n",
    "                    best_std   = []\n",
    "                    best_comb  = []\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "                elif score_mean == best_score[0]:\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "            else:\n",
    "                best_score.append(score_mean)\n",
    "                best_std.append(score_std)\n",
    "                best_comb.append(comb)\n",
    "num_ties = len(best_score)\n",
    "for ind in range(num_ties):\n",
    "    print 'For comb=%s, Support Vector Classifier Accuracy = %f +/- %f' \\\n",
    "            % (best_comb[ind],best_score[ind],best_std[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimize decision tree classifier\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "best_score = []\n",
    "best_std   = []\n",
    "best_comb  = []\n",
    "nfeatures  = 18\n",
    "iterable   = range(nfeatures)\n",
    "model      = DecisionTreeClassifier()\n",
    "for s in xrange(len(iterable)+1):\n",
    "    for comb in itertools.combinations(iterable, s):\n",
    "        if len(comb) > 0:\n",
    "            Xsel = []\n",
    "            for patient in Xall:\n",
    "                Xsel.append([patient[ind] for ind in comb])\n",
    "            this_scores = cross_val_score(model, Xsel, y=yall, cv=3 )\n",
    "            score_mean  = np.mean(this_scores)\n",
    "            score_std   = np.std(this_scores)\n",
    "            if len(best_score) > 0: \n",
    "                if score_mean > best_score[0]:\n",
    "                    best_score = []\n",
    "                    best_std   = []\n",
    "                    best_comb  = []\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "                elif score_mean == best_score[0]:\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "            else:\n",
    "                best_score.append(score_mean)\n",
    "                best_std.append(score_std)\n",
    "                best_comb.append(comb)\n",
    "num_ties = len(best_score)\n",
    "for ind in range(num_ties):\n",
    "    print 'For comb=%s, Decision Tree Classifier Accuracy = %f +/- %f' \\\n",
    "            % (best_comb[ind],best_score[ind],best_std[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimize random forest classifier\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "best_score = []\n",
    "best_std   = []\n",
    "best_comb  = []\n",
    "nfeatures  = 18\n",
    "iterable   = range(nfeatures)\n",
    "model      = RandomForestClassifier()\n",
    "for s in xrange(len(iterable)+1):\n",
    "    for comb in itertools.combinations(iterable, s):\n",
    "        if len(comb) > 0:\n",
    "            Xsel = []\n",
    "            for patient in Xall:\n",
    "                Xsel.append([patient[ind] for ind in comb])\n",
    "            this_scores = cross_val_score(model, Xsel, y=yall, cv=3 )\n",
    "            score_mean  = np.mean(this_scores)\n",
    "            score_std   = np.std(this_scores)\n",
    "            if len(best_score) > 0: \n",
    "                if score_mean > best_score[0]:\n",
    "                    best_score = []\n",
    "                    best_std   = []\n",
    "                    best_comb  = []\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "                elif score_mean == best_score[0]:\n",
    "                    best_score.append(score_mean)\n",
    "                    best_std.append(score_std)\n",
    "                    best_comb.append(comb)\n",
    "            else:\n",
    "                best_score.append(score_mean)\n",
    "                best_std.append(score_std)\n",
    "                best_comb.append(comb)\n",
    "num_ties = len(best_score)\n",
    "for ind in range(num_ties):\n",
    "    print 'For comb=%s, Random Forest Classifier Accuracy = %f +/- %f' \\\n",
    "            % (best_comb[ind],best_score[ind],best_std[ind])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
